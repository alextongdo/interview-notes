## Machine Learning Papers

### [Correlation and Navigation in the Vocabulary Key Representation Space of Language Models](https://arxiv.org/pdf/2410.02284)

- Abstract: Transformer models doing next token prediction calculate the next token's distribution by performing a dot product between the encoded input (query vector) and fixed vocabulary representations (keys). 